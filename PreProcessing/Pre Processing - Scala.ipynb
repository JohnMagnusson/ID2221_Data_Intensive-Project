{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions.{min, max, mean, stddev, typedLit, when}\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "println(\"all good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is in the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TradingInfo(1595581200,9536.29,9514.4,9514.87,1017.91,9698013.33,9536.26,\\direct\\,\\\\)\n",
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class TradingInfo\n",
       "filename = ../Datasets/bitcoin_trading_data.txt\n",
       "tradingInformation = MapPartitionsRDD[101] at map at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createTradingInfo: (data: String)TradingInfo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[101] at map at <console>:48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class TradingInfo (time: Int,\n",
    "                        high: Double,\n",
    "                        low: Double,\n",
    "                        open: Double,\n",
    "                        volumefrom: Double,\n",
    "                        volumeto: Double, \n",
    "                        close: Double,\n",
    "                        conversionType: String,\n",
    "                        conversionSymbol: String                        \n",
    "                       )\n",
    "\n",
    "def createTradingInfo(data: String): TradingInfo = {\n",
    "    val s = data.split(\", \")\n",
    "    return TradingInfo(s(0).split(\": \")(1).toInt,    s(1).split(\": \")(1).toDouble, s(2).split(\": \")(1).toDouble, \n",
    "                       s(3).split(\": \")(1).toDouble, s(4).split(\": \")(1).toDouble, s(5).split(\": \")(1).toDouble, \n",
    "                       s(6).split(\": \")(1).toDouble, s(7).split(\": \")(1),          s(8).split(\": \")(1))\n",
    "}\n",
    "\n",
    "val filename = \"../Datasets/bitcoin_trading_data.txt\"\n",
    "val tradingInformation = sc.textFile(filename)\n",
    "            .flatMap(l => l.split(\"]}},\"))       // Splits into indiviual http calls\n",
    "            .map(x => x.split(\"\"\": \\[\\{\"\"\")(1))  // Splits up between header info and data payload\n",
    "            .flatMap(x => x.split(\"},\"))         // Now we have row wise data. It is bit messy so need to clean it\n",
    "            .map(x => x.replace(\"{\", \"\"))\n",
    "            .map(x => x.replace(\"\\\"\", \"\"))\n",
    "            .map(x => x.replace(\"]}}]\", \"\"))     // Last response is special case\n",
    "            .map(createTradingInfo)\n",
    "            .cache()\n",
    "\n",
    "tradingInformation.take(1).foreach(println)\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [time: int, high: double ... 7 more fields]\n",
       "df1 = [time: int, high: double ... 8 more fields]\n",
       "ratio = 0.9\n",
       "training_size = 12602.7\n",
       "training_position = 1597734000\n",
       "training_set = [time: int, high: double ... 8 more fields]\n",
       "validation_set = [time: int, high: double ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 8 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Data Action\n",
    "\n",
    "val df = spark.createDataFrame(tradingInformation).sort(\"time\")\n",
    "val df1 = df.withColumn(\"midPrice\",($\"high\" + $\"low\")/2)\n",
    "\n",
    "val ratio: Double = 0.9\n",
    "val training_size = df1.count*ratio\n",
    "\n",
    "val training_position = df1.take(training_size.toInt).last.getInt(0)\n",
    "\n",
    "val training_set = df1.filter($\"time\" <= training_position).cache()\n",
    "val validation_set = df1.filter($\"time\" > training_position).cache()\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "take_statistics_df: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good\n"
     ]
    }
   ],
   "source": [
    "def take_statistics_df(df: DataFrame): DataFrame = {\n",
    "    val statistics = df.agg(mean(\"midPrice\"), stddev(\"midPrice\"), min(\"midPrice\"), max(\"midPrice\"),\n",
    "                            mean(\"volumefrom\"), stddev(\"volumefrom\"), min(\"volumefrom\"), max(\"volumefrom\"),\n",
    "                            mean(\"volumeto\"), stddev(\"volumeto\"), min(\"volumeto\"), max(\"volumeto\")).head()\n",
    "    val midPrice_mean = statistics.getDouble(0)\n",
    "    val midPrice_std = statistics.getDouble(1)\n",
    "    val midPrice_min = statistics.getDouble(2)\n",
    "    val midPrice_max = statistics.getDouble(3)\n",
    "    val midPrice_range = midPrice_max - midPrice_min\n",
    "\n",
    "    // // VolumeFrom\n",
    "    val volumefrom_mean = statistics.getDouble(4)\n",
    "    val volumefrom_std = statistics.getDouble(5)\n",
    "    val volumefrom_min = statistics.getDouble(6)\n",
    "    val volumefrom_max = statistics.getDouble(7)\n",
    "    val volumefrom_range = volumefrom_max - volumefrom_min\n",
    "\n",
    "    // // VolumeTo\n",
    "    val volumeto_mean = statistics.getDouble(8)\n",
    "    val volumeto_std = statistics.getDouble(9)\n",
    "    val volumeto_min = statistics.getDouble(10)\n",
    "    val volumeto_max = statistics.getDouble(11)\n",
    "    val volumeto_range = volumeto_max - volumeto_min\n",
    "\n",
    "    val new_dataset = df.withColumn(\"midPriceNorm\",(($\"midPrice\" - midPrice_min)/midPrice_range).cast(DoubleType))\n",
    "                   .withColumn(\"volumefromNorm\",(($\"volumefrom\" - volumefrom_min)/volumefrom_range).cast(DoubleType))\n",
    "                   .withColumn(\"volumetoNorm\",(($\"volumeto\" - volumeto_min)/volumeto_range).cast(DoubleType))\n",
    "                   .as(\"df_2\")\n",
    "\n",
    "    \n",
    "    return new_dataset\n",
    "}\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training_df = [time: int, high: double ... 11 more fields]\n",
       "validation_df = [time: int, high: double ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 11 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training_df = take_statistics_df(training_set).cache()\n",
    "val validation_df = take_statistics_df(validation_set).cache()\n",
    "\n",
    "println(\"all good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_path_training = cleaned_data/training_set\n",
       "save_training_df = [time: int, high: double ... 12 more fields]\n",
       "data_path_validation = cleaned_data/validation_set\n",
       "save_validation_df = [time: int, high: double ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 12 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Saving data as Text\n",
    "// don't forget to delete the previous folder \n",
    "val data_path_training = \"cleaned_data/training_set\"\n",
    "val save_training_df = training_df.withColumn(\"empty\", typedLit(Seq(1)))\n",
    "save_training_df.rdd.repartition(1).saveAsTextFile(data_path_training)\n",
    "\n",
    "val data_path_validation = \"cleaned_data/validation_set\"\n",
    "val save_validation_df = validation_df.withColumn(\"empty\", typedLit(Seq(1)))\n",
    "save_validation_df.rdd.repartition(1).saveAsTextFile(data_path_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "take_statistics_social: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "def take_statistics_social(df: DataFrame): DataFrame = {\n",
    "        val statistics = df.agg(mean(\"total_page_views\"), stddev(\"total_page_views\"), min(\"total_page_views\"), max(\"total_page_views\"),\n",
    "                                     mean(\"fb_talking_about\"), stddev(\"fb_talking_about\"), min(\"fb_talking_about\"), max(\"fb_talking_about\"),\n",
    "                                     mean(\"reddit_posts_per_hour\"), stddev(\"reddit_posts_per_hour\"), min(\"reddit_posts_per_hour\"), max(\"reddit_posts_per_hour\"), \n",
    "                                     mean(\"reddit_comments_per_hour\"), stddev(\"reddit_comments_per_hour\"), min(\"reddit_comments_per_hour\"), max(\"reddit_comments_per_hour\")).head()\n",
    "\n",
    "        val pageViews_mean = statistics.getDouble(0)\n",
    "        val pageViews_std = statistics.getDouble(1)\n",
    "        val pageViews_min = statistics.getInt(2)\n",
    "        val pageViews_max = statistics.getInt(3)\n",
    "        val pageViews_range = pageViews_max - pageViews_min\n",
    "\n",
    "        // // fbTalking\n",
    "        val fbTalking_mean = statistics.getDouble(4)\n",
    "        val fbTalking_std = statistics.getDouble(5)\n",
    "        val fbTalking_min = statistics.getInt(6)\n",
    "        val fbTalking_max = statistics.getInt(7)\n",
    "        val fbTalking_range = fbTalking_max - fbTalking_min\n",
    "\n",
    "        // // redditPosts\n",
    "        val redditPosts_mean = statistics.getDouble(8)\n",
    "        val redditPosts_std = statistics.getDouble(9)\n",
    "        val redditPosts_min = statistics.getDouble(10)\n",
    "        val redditPosts_max = statistics.getDouble(11)\n",
    "        val redditPosts_range = redditPosts_max - redditPosts_min\n",
    "\n",
    "        val redditComments_mean = statistics.getDouble(12)\n",
    "        val redditComments_std = statistics.getDouble(13)\n",
    "        val redditComments_min = statistics.getDouble(14)\n",
    "        val redditComments_max = statistics.getDouble(15)\n",
    "        val redditComments_range = redditComments_max - redditComments_min\n",
    "\n",
    "\n",
    "        val new_df = df.withColumn(\"pageViewsNorm\",(($\"total_page_views\" - pageViews_min)/pageViews_range).cast(DoubleType))\n",
    "                       .withColumn(\"fbTalkingNorm\",(($\"fb_talking_about\" - fbTalking_min)/fbTalking_range).cast(DoubleType))\n",
    "                       .withColumn(\"redditPostsNorm\",(($\"reddit_posts_per_hour\" - redditPosts_min)/redditPosts_range).cast(DoubleType))\n",
    "                       .withColumn(\"redditCommentsNorm\",(($\"reddit_comments_per_hour\" - redditComments_min)/redditComments_range).cast(DoubleType))\n",
    "    \n",
    "    return new_df\n",
    "}\n",
    "\n",
    "println(\"all good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: integer (nullable = false)\n",
      " |-- high: double (nullable = false)\n",
      " |-- low: double (nullable = false)\n",
      " |-- open: double (nullable = false)\n",
      " |-- volumefrom: double (nullable = false)\n",
      " |-- volumeto: double (nullable = false)\n",
      " |-- close: double (nullable = false)\n",
      " |-- conversionType: string (nullable = true)\n",
      " |-- conversionSymbol: string (nullable = true)\n",
      " |-- midPrice: double (nullable = true)\n",
      " |-- midPriceNorm: double (nullable = true)\n",
      " |-- volumefromNorm: double (nullable = true)\n",
      " |-- volumetoNorm: double (nullable = true)\n",
      " |-- total_page_views: integer (nullable = false)\n",
      " |-- fb_talking_about: integer (nullable = false)\n",
      " |-- reddit_posts_per_hour: double (nullable = false)\n",
      " |-- reddit_comments_per_hour: double (nullable = false)\n",
      " |-- pageViewsNorm: double (nullable = true)\n",
      " |-- fbTalkingNorm: double (nullable = true)\n",
      " |-- redditPostsNorm: double (nullable = true)\n",
      " |-- redditCommentsNorm: double (nullable = true)\n",
      "\n",
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class SocialData\n",
       "filename_social = ../Datasets/bitcoin_social_data.txt\n",
       "socialDataInformation = MapPartitionsRDD[282] at map at <console>:105\n",
       "socialDataDf = [time: int, comments: int ... 32 more fields]\n",
       "socialDatadf_shorten = [time: int, total_page_views: int ... 3 more fields]\n",
       "norm_social_df = [time: int, total_page_views: int ... 7 more fields]\n",
       "mergedDataDf_training = [time: int, high: double ... 19 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createSocialData: (data: String)SocialData\n",
       "extractAndConvertToInt: (attribute: String)Int\n",
       "extractAndConvertToDouble: (attribute: String)Double\n",
       "mergedD...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 19 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Social data is a colelction of various values from different social platforms \n",
    "// Explanation of the values can be found at: https://min-api.cryptocompare.com/documentation?key=Social&cat=historicalHourSocialStats\n",
    "\n",
    "\n",
    "case class SocialData (time: Int,\n",
    "                comments: Int,\n",
    "                posts: Int,\n",
    "                followers: Int,\n",
    "                points: Int,\n",
    "                overview_page_views: Int,\n",
    "                analysis_page_views: Int,\n",
    "                markets_page_views: Int,\n",
    "                charts_page_views: Int,\n",
    "                trades_page_views: Int, \n",
    "                forum_page_views: Int, \n",
    "                influence_page_views: Int, \n",
    "                total_page_views: Int, //\n",
    "                fb_likes: Int,\n",
    "                fb_talking_about: Int, //\n",
    "                twitter_followers: Int,\n",
    "                twitter_following: Int,\n",
    "                twitter_lists: Int,\n",
    "                twitter_favourites: Int,\n",
    "                twitter_statuses: Int,\n",
    "                reddit_subscribers: Int,\n",
    "                reddit_active_users: Int,\n",
    "                reddit_posts_per_hour: Double, //\n",
    "                reddit_posts_per_day: Double,\n",
    "                reddit_comments_per_hour: Double, //\n",
    "                reddit_comments_per_day: Double,\n",
    "                code_repo_stars: Int,\n",
    "                code_repo_forks: Int,\n",
    "                code_repo_subscribers: Int,\n",
    "                code_repo_open_pull_issues: Int,\n",
    "                code_repo_closed_pull_issues: Int,\n",
    "                code_repo_open_issues: Int,\n",
    "                code_repo_closed_issues: Int,\n",
    "                code_repo_contributors: Int                     \n",
    "               )\n",
    "\n",
    "def createSocialData(data: String): SocialData = {\n",
    "val s = data.split(\", \")\n",
    "return SocialData(extractAndConvertToInt(s(0)),    extractAndConvertToInt(s(1)),     extractAndConvertToInt(s(2)),    extractAndConvertToInt(s(3)),\n",
    "              extractAndConvertToInt(s(4)),    extractAndConvertToInt(s(5)),     extractAndConvertToInt(s(6)),    extractAndConvertToInt(s(7)),\n",
    "              extractAndConvertToInt(s(8)),    extractAndConvertToInt(s(9)),     extractAndConvertToInt(s(10)),   extractAndConvertToInt(s(11)),\n",
    "              extractAndConvertToInt(s(12)),   extractAndConvertToInt(s(13)),    extractAndConvertToInt(s(14)),   extractAndConvertToInt(s(15)),\n",
    "              extractAndConvertToInt(s(16)),   extractAndConvertToInt(s(17)),    extractAndConvertToInt(s(18)),   extractAndConvertToInt(s(19)),\n",
    "              extractAndConvertToInt(s(20)),   extractAndConvertToInt(s(21)),    extractAndConvertToDouble(s(22)),extractAndConvertToDouble(s(23)),\n",
    "              extractAndConvertToDouble(s(24)),extractAndConvertToDouble(s(25)), extractAndConvertToInt(s(26)),   extractAndConvertToInt(s(27)),\n",
    "              extractAndConvertToInt(s(28)),   extractAndConvertToInt(s(29)),    extractAndConvertToInt(s(30)),   extractAndConvertToInt(s(31)),\n",
    "              extractAndConvertToInt(s(32)),   extractAndConvertToInt(s(33)))\n",
    "}\n",
    "\n",
    "def extractAndConvertToInt(attribute: String): Int = {\n",
    "return attribute.split(\": \")(1).toInt\n",
    "}\n",
    "\n",
    "def extractAndConvertToDouble(attribute: String): Double = {\n",
    "return attribute.split(\": \")(1).toDouble\n",
    "}\n",
    "\n",
    "\n",
    "val filename_social = \"../Datasets/bitcoin_social_data.txt\"\n",
    "val socialDataInformation = sc.textFile(filename_social)\n",
    "    .flatMap(l => l.split(\"]},\"))         // Splits into indiviual http calls\n",
    "    .map(x => x.split(\"\"\": \\[\\{\"\"\")(1))   // Splits up between header info and data payload\n",
    "    .flatMap(x => x.split(\"},\"))          // Now we have row wise data. It is bit messy so need to clean it\n",
    "    .map(x => x.replace(\"{\", \"\"))\n",
    "    .map(x => x.replace(\"\\\"\", \"\"))\n",
    "    .map(x => x.replace(\"}\", \"\"))\n",
    "    .map(x => x.replace(\"]]\", \"\"))     // Last response is special case\n",
    "    .map(createSocialData)\n",
    "    .cache()    \n",
    "\n",
    "    //         socialDataInformation.take(1).foreach(println)\n",
    "\n",
    "// Merge TradingInfo with SocialData\n",
    "\n",
    "var socialDataDf = spark.createDataFrame(socialDataInformation).sort(\"time\")\n",
    "val socialDatadf_shorten = socialDataDf.select(\"time\", \"total_page_views\", \"fb_talking_about\", \"reddit_posts_per_hour\", \"reddit_comments_per_hour\")\n",
    "val norm_social_df = take_statistics_social(socialDatadf_shorten)\n",
    "\n",
    "val mergedDataDf_training = training_df.join(norm_social_df, Seq(\"time\")).cache()\n",
    "val mergedDataDf_validation = validation_df.join(norm_social_df, Seq(\"time\")).cache()\n",
    "mergedDataDf_training.printSchema()\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_path_training_social = cleaned_data_socialMedia/training_set\n",
       "save_training_df_social = [time: int, high: double ... 20 more fields]\n",
       "data_path_validation_social = cleaned_data_socialMedia/validation_set\n",
       "save_validation_df_social = [time: int, high: double ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 20 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Saving data as Text\n",
    "// don't forget to delete the previous folder \n",
    "val data_path_training_social = \"cleaned_data_socialMedia/training_set\"\n",
    "val save_training_df_social = mergedDataDf_training.withColumn(\"empty\", typedLit(Seq(1)))\n",
    "save_training_df_social.rdd.repartition(1).saveAsTextFile(data_path_training_social)\n",
    "\n",
    "val data_path_validation_social = \"cleaned_data_socialMedia/validation_set\"\n",
    "val save_validation_df_social = mergedDataDf_validation.withColumn(\"empty\", typedLit(Seq(1)))\n",
    "save_validation_df_social.rdd.repartition(1).saveAsTextFile(data_path_validation_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
