{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions.{min, max, mean, stddev, typedLit, when}\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType}\n",
    "\n",
    "println(\"all good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is in the format:\n",
    "\n",
    "{\n",
    "\"conversionSymbol\": \"\",\n",
    "\"volumeto\": 6685400.72, \n",
    "\"high\": 9165.89, \n",
    "\"low\": 9144.6, \n",
    "\"time\": 1595030400, \n",
    "\"volumefrom\": 730.39, \n",
    "\"close\": 9151.68, \n",
    "\"open\": 9156.79, \n",
    "\"conversionType\": \"direct\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TradingInfo(1595574000,9516.47,9477.19,9484.19,1619.05,1.537644208E7,9516.47,\\direct\\,\\\\)\n",
      "TradingInfo(1595577600,9518.18,9494.68,9516.47,1147.74,1.09079891E7,9514.87,\\direct\\,\\\\)\n",
      "TradingInfo(1595581200,9536.29,9514.4,9514.87,1017.91,9698013.33,9536.26,\\direct\\,\\\\)\n",
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class TradingInfo\n",
       "filename = ../Datasets/bitcoin_data_max.txt\n",
       "tradingInformation = MapPartitionsRDD[193] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createTradingInfo: (data: String)TradingInfo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[193] at map at <console>:47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class TradingInfo (time: Int,\n",
    "                        high: Double,\n",
    "                        low: Double,\n",
    "                        open: Double,\n",
    "                        volumefrom: Double,\n",
    "                        volumeto: Double, \n",
    "                        close: Double,\n",
    "                        conversionType: String,\n",
    "                        conversionSymbol: String                        \n",
    "                       )\n",
    "\n",
    "def createTradingInfo(data: String): TradingInfo = {\n",
    "    val s = data.split(\", \")\n",
    "    return TradingInfo(s(0).split(\": \")(1).toInt, s(1).split(\": \")(1).toDouble, s(2).split(\": \")(1).toDouble, \n",
    "                       s(3).split(\": \")(1).toDouble, s(4).split(\": \")(1).toDouble, s(5).split(\": \")(1).toDouble, \n",
    "                       s(6).split(\": \")(1).toDouble, s(7).split(\": \")(1), s(8).split(\": \")(1))\n",
    "}\n",
    "\n",
    "// val filename = \"../Datasets/bitcoin_json.txt\"\n",
    "val filename = \"../Datasets/bitcoin_data_max.txt\"\n",
    "val tradingInformation = sc.textFile(filename).\n",
    "            flatMap(l => l.split(\"},\")).\n",
    "            map(x => x.replace(\"[\", \"\")).\n",
    "            map(x => x.replace(\"{\", \"\")).\n",
    "            map(x => x.replace(\"\\\"\", \"\")).\n",
    "            map(createTradingInfo)\n",
    "            .cache()\n",
    "\n",
    "tradingInformation.take(3).foreach(println)\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [time: int, high: double ... 7 more fields]\n",
       "df1 = [time: int, high: double ... 8 more fields]\n",
       "ratio = 0.9\n",
       "training_size = 12602.7\n",
       "testing_size = 13993\n",
       "training_position = 1597726800\n",
       "testing_position = 1602734400\n",
       "training_set = [time: int, high: double ... 8 more fields]\n",
       "validation_set = [time: int, high: double ... 8 more fields]\n",
       "testing_set = [time: int, high: double ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 8 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Data Action\n",
    "\n",
    "val df = spark.createDataFrame(tradingInformation).sort(\"time\")\n",
    "val df1 = df.withColumn(\"midPrice\",($\"high\" + $\"low\")/2)\n",
    "\n",
    "// df1.show(5)\n",
    "\n",
    "val ratio: Double = 0.9\n",
    "val training_size = df1.count*ratio\n",
    "val testing_size = df1.count - 10\n",
    "\n",
    "val training_position = df1.take(training_size.toInt).last.getInt(0)\n",
    "val testing_position = df1.take(testing_size.toInt).last.getInt(0)\n",
    "\n",
    "// val df2 = df1.withColumn(\"dataset\", when(col(\"time\") <= training_position,\"training\")\n",
    "//       .when(col(\"time\") > training_position and col(\"time\") <= testing_position,\"validation\")\n",
    "//       .otherwise(\"testing\"))\n",
    "// df2.show(5)\n",
    "\n",
    "\n",
    "val training_set = df1.filter($\"time\" <= training_position).cache()\n",
    "val validation_set = df1.filter($\"time\" > training_position).filter($\"time\" <= testing_position).cache()\n",
    "val testing_set = df1.filter($\"time\" > testing_position).cache()\n",
    "\n",
    "\n",
    "// training_set.show(5)\n",
    "// validation_set.show(5)\n",
    "// testing_set.show(5)\n",
    "\n",
    "\n",
    "// println(testing_set.count)\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+----------+-------------+-------+--------------+----------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      time|   high|    low|   open|volumefrom|     volumeto|  close|conversionType|conversionSymbol|          midPrice|        midPriceNorm|      volumefromNorm|        volumetoNorm|\n",
      "+----------+-------+-------+-------+----------+-------------+-------+--------------+----------------+------------------+--------------------+--------------------+--------------------+\n",
      "|1552370400|3847.36|3841.99|3843.46|   1150.58|   4412932.61|3846.66|      \\direct\\|              \\\\|          3844.675|                 0.0|0.007802768998301...|0.005782282635044576|\n",
      "|1552374000| 3852.5|3846.16|3846.66|    1551.8|   5984981.18|3850.52|      \\direct\\|              \\\\|           3849.33|4.730469917056464...|0.010524369849756021| 0.00784282947856672|\n",
      "|1552377600|3886.31|3849.94|3850.52|   3193.81|1.238809879E7|3884.04|      \\direct\\|            \\\\}]|          3868.125|0.002383018680021...|0.021662637668151817| 0.01623565145097344|\n",
      "|1552381200|3888.46| 3879.2|3884.04|   1501.41|   5817773.43|3885.09|      \\direct\\|              \\\\|           3883.83|0.003978980657408267|0.010182558706481033| 0.00762366360287438|\n",
      "|1552384800|3896.92|3884.47|3885.09|   1468.94|   5703460.72|3895.69|      \\direct\\|              \\\\|3890.6949999999997|0.004676610646250229|0.009962304532594569|0.007473829368576541|\n",
      "+----------+-------+-------+-------+----------+-------------+-------+--------------+----------------+------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- time: integer (nullable = false)\n",
      " |-- high: double (nullable = false)\n",
      " |-- low: double (nullable = false)\n",
      " |-- open: double (nullable = false)\n",
      " |-- volumefrom: double (nullable = false)\n",
      " |-- volumeto: double (nullable = false)\n",
      " |-- close: double (nullable = false)\n",
      " |-- conversionType: string (nullable = true)\n",
      " |-- conversionSymbol: string (nullable = true)\n",
      " |-- midPrice: double (nullable = true)\n",
      " |-- midPriceNorm: double (nullable = true)\n",
      " |-- volumefromNorm: double (nullable = true)\n",
      " |-- volumetoNorm: double (nullable = true)\n",
      "\n",
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_type = cleaned_data_socialMedia/training_set\n",
       "dataset = [time: int, high: double ... 8 more fields]\n",
       "statistics = [8503.246020472938,1906.3696808766083,3844.675,13685.135,2024.5121378511346,2739.3449201142676,0.2912,147420.88,1.713158637164259E7,2.1539473974606913E7,1468.16,7.6292931663E8]\n",
       "midPrice_mean = 8503.246020472938\n",
       "midPrice_std = 1906.3696808766083\n",
       "midPrice_min = 3844.675\n",
       "midPrice_max = 13685.135\n",
       "midPrice_range = 9840.46\n",
       "volumefrom_mean = 2024.5121378511346\n",
       "volumefrom_std = 2739.3449201142676\n",
       "volumefrom_min = 0.2912\n",
       "volumefrom_max = 147420.88\n",
       "volumefrom_range = 147420.5888\n",
       "volumeto_mean = 1.7131586371...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.7131586371..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// training_set\n",
    "// validation_set\n",
    "// testing_set\n",
    "val data_type = \"cleaned_data_socialMedia/training_set\"\n",
    "val dataset = training_set\n",
    "\n",
    "val statistics = dataset.agg(mean(\"midPrice\"), stddev(\"midPrice\"), min(\"midPrice\"), max(\"midPrice\"),\n",
    "                                  mean(\"volumefrom\"), stddev(\"volumefrom\"), min(\"volumefrom\"), max(\"volumefrom\"),\n",
    "                                  mean(\"volumeto\"), stddev(\"volumeto\"), min(\"volumeto\"), max(\"volumeto\")).head()\n",
    "\n",
    "val midPrice_mean = statistics.getDouble(0)\n",
    "val midPrice_std = statistics.getDouble(1)\n",
    "val midPrice_min = statistics.getDouble(2)\n",
    "val midPrice_max = statistics.getDouble(3)\n",
    "val midPrice_range = midPrice_max - midPrice_min\n",
    "\n",
    "// // VolumeFrom\n",
    "val volumefrom_mean = statistics.getDouble(4)\n",
    "val volumefrom_std = statistics.getDouble(5)\n",
    "val volumefrom_min = statistics.getDouble(6)\n",
    "val volumefrom_max = statistics.getDouble(7)\n",
    "val volumefrom_range = volumefrom_max - volumefrom_min\n",
    "\n",
    "// // VolumeTo\n",
    "val volumeto_mean = statistics.getDouble(8)\n",
    "val volumeto_std = statistics.getDouble(9)\n",
    "val volumeto_min = statistics.getDouble(10)\n",
    "val volumeto_max = statistics.getDouble(11)\n",
    "val volumeto_range = volumeto_max - volumeto_min\n",
    "\n",
    "// // // from -1 to 1 \n",
    "// // //val new_dataset = dataset.withColumn(\"midPriceNorm\",($\"midPrice\" - midPrice_mean)/midPrice_std)\n",
    "// // // from 0 to 1 \n",
    "val new_dataset = dataset.withColumn(\"midPriceNorm\",(($\"midPrice\" - midPrice_min)/midPrice_range).cast(DoubleType))\n",
    "               .withColumn(\"volumefromNorm\",(($\"volumefrom\" - volumefrom_min)/volumefrom_range).cast(DoubleType))\n",
    "               .withColumn(\"volumetoNorm\",(($\"volumeto\" - volumeto_min)/volumeto_range).cast(DoubleType))\n",
    "               .cache()\n",
    "               .as(\"df_2\")\n",
    "\n",
    "new_dataset.show(5)\n",
    "new_dataset.printSchema()\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Saving data as Text for complete bitcoin timeframe without socialData\n",
    "// save it 3 times, one for each data set (training / validation / testing )\n",
    "// don't forget to delete the previous folder \n",
    "val pre_processed_dataset = new_dataset.withColumn(\"empty\", typedLit(Seq(1)))\n",
    "pre_processed_dataset.rdd.repartition(1).saveAsTextFile(data_type)\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SocialData(1595574000,356082,120973,78756,8554045,30012388,1094231,1600257,9444371,800616,9097523,62746,52112132,39654,26,844049,165,6631,1000,20316,1523948,3900,4.32,103.63,60.44,1450.64,44258,26102,3500,387,13297,543,5043,891)\n",
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class SocialData\n",
       "filename = ../Datasets/bitcoin_social_max.txt\n",
       "socialDataInformation = MapPartitionsRDD[477] at map at <console>:92\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createSocialData: (data: String)SocialData\n",
       "extractAndConvertToInt: (attribute: String)Int\n",
       "extractAndConvertToDouble: (attribute: String)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[477] at map at <console>:92"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Social data is a colelction of various values from different social platforms \n",
    "// Explanation of the values can be found at: https://min-api.cryptocompare.com/documentation?key=Social&cat=historicalHourSocialStats\n",
    "case class SocialData (time: Int,\n",
    "                        comments: Int,\n",
    "                        posts: Int,\n",
    "                        followers: Int,\n",
    "                        points: Int,\n",
    "                        overview_page_views: Int,\n",
    "                        analysis_page_views: Int,\n",
    "                        markets_page_views: Int,\n",
    "                        charts_page_views: Int,\n",
    "                        trades_page_views: Int,\n",
    "                        forum_page_views: Int,\n",
    "                        influence_page_views: Int,\n",
    "                        total_page_views: Int,\n",
    "                        fb_likes: Int,\n",
    "                        fb_talking_about: Int,\n",
    "                        twitter_followers: Int,\n",
    "                        twitter_following: Int,\n",
    "                        twitter_lists: Int,\n",
    "                        twitter_favourites: Int,\n",
    "                        twitter_statuses: Int,\n",
    "                        reddit_subscribers: Int,\n",
    "                        reddit_active_users: Int,\n",
    "                        reddit_posts_per_hour: Double,\n",
    "                        reddit_posts_per_day: Double,\n",
    "                        reddit_comments_per_hour: Double,\n",
    "                        reddit_comments_per_day: Double,\n",
    "                        code_repo_stars: Int,\n",
    "                        code_repo_forks: Int,\n",
    "                        code_repo_subscribers: Int,\n",
    "                        code_repo_open_pull_issues: Int,\n",
    "                        code_repo_closed_pull_issues: Int,\n",
    "                        code_repo_open_issues: Int,\n",
    "                        code_repo_closed_issues: Int,\n",
    "                        code_repo_contributors: Int                     \n",
    "                       )\n",
    "\n",
    "def createSocialData(data: String): SocialData = {\n",
    "    val s = data.split(\", \")\n",
    "    return SocialData(extractAndConvertToInt(s(0)),    extractAndConvertToInt(s(1)),     extractAndConvertToInt(s(2)),    extractAndConvertToInt(s(3)),\n",
    "                      extractAndConvertToInt(s(4)),    extractAndConvertToInt(s(5)),     extractAndConvertToInt(s(6)),    extractAndConvertToInt(s(7)),\n",
    "                      extractAndConvertToInt(s(8)),    extractAndConvertToInt(s(9)),     extractAndConvertToInt(s(10)),   extractAndConvertToInt(s(11)),\n",
    "                      extractAndConvertToInt(s(12)),   extractAndConvertToInt(s(13)),    extractAndConvertToInt(s(14)),   extractAndConvertToInt(s(15)),\n",
    "                      extractAndConvertToInt(s(16)),   extractAndConvertToInt(s(17)),    extractAndConvertToInt(s(18)),   extractAndConvertToInt(s(19)),\n",
    "                      extractAndConvertToInt(s(20)),   extractAndConvertToInt(s(21)),    extractAndConvertToDouble(s(22)),extractAndConvertToDouble(s(23)),\n",
    "                      extractAndConvertToDouble(s(24)),extractAndConvertToDouble(s(25)), extractAndConvertToInt(s(26)),   extractAndConvertToInt(s(27)),\n",
    "                      extractAndConvertToInt(s(28)),   extractAndConvertToInt(s(29)),    extractAndConvertToInt(s(30)),   extractAndConvertToInt(s(31)),\n",
    "                      extractAndConvertToInt(s(32)),   extractAndConvertToInt(s(33)))\n",
    "}\n",
    "\n",
    "def extractAndConvertToInt(attribute: String): Int = {\n",
    "    return attribute.split(\": \")(1).toInt\n",
    "}\n",
    "\n",
    "def extractAndConvertToDouble(attribute: String): Double = {\n",
    "    return attribute.split(\": \")(1).toDouble\n",
    "}\n",
    "\n",
    "\n",
    "val filename = \"../Datasets/bitcoin_social_max.txt\"\n",
    "val socialDataInformation = sc.textFile(filename)\n",
    "            .flatMap(l => l.split(\"},\"))\n",
    "            //.map(x => x.replace(\"\\\\\", \"\"))\n",
    "            .map(x => x.replace(\"[\", \"\"))\n",
    "            .map(x => x.replace(\"{\", \"\"))\n",
    "            .map(x => x.replace(\"\\\"\", \"\"))\n",
    "            .map(x => x.replace(\"]\", \"\")) // Why do i need this here and not in the previous parser?\n",
    "            .map(x => x.replace(\"}\", \"\")) // -||-\n",
    "            .map(createSocialData)\n",
    "            .cache()\n",
    "\n",
    "            \n",
    "\n",
    "socialDataInformation.take(1).foreach(println)\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+----------+----------+-------+--------------+----------------+--------+------------+--------------------+--------------------+--------+-----+---------+-------+-------------------+-------------------+------------------+-----------------+-----------------+----------------+--------------------+----------------+--------+----------------+-----------------+-----------------+-------------+------------------+----------------+------------------+-------------------+---------------------+--------------------+------------------------+-----------------------+---------------+---------------+---------------------+--------------------------+----------------------------+---------------------+-----------------------+----------------------+\n",
      "|      time|   high|    low|   open|volumefrom|  volumeto|  close|conversionType|conversionSymbol|midPrice|midPriceNorm|      volumefromNorm|        volumetoNorm|comments|posts|followers| points|overview_page_views|analysis_page_views|markets_page_views|charts_page_views|trades_page_views|forum_page_views|influence_page_views|total_page_views|fb_likes|fb_talking_about|twitter_followers|twitter_following|twitter_lists|twitter_favourites|twitter_statuses|reddit_subscribers|reddit_active_users|reddit_posts_per_hour|reddit_posts_per_day|reddit_comments_per_hour|reddit_comments_per_day|code_repo_stars|code_repo_forks|code_repo_subscribers|code_repo_open_pull_issues|code_repo_closed_pull_issues|code_repo_open_issues|code_repo_closed_issues|code_repo_contributors|\n",
      "+----------+-------+-------+-------+----------+----------+-------+--------------+----------------+--------+------------+--------------------+--------------------+--------+-----+---------+-------+-------------------+-------------------+------------------+-----------------+-----------------+----------------+--------------------+----------------+--------+----------------+-----------------+-----------------+-------------+------------------+----------------+------------------+-------------------+---------------------+--------------------+------------------------+-----------------------+---------------+---------------+---------------------+--------------------------+----------------------------+---------------------+-----------------------+----------------------+\n",
      "|1552370400|3847.36|3841.99|3843.46|   1150.58|4412932.61|3846.66|      \\direct\\|              \\\\|3844.675|         0.0|0.007802768998301...|0.005782282635044576|  242738|92603|    65331|6375730|           22843253|             969000|           1419690|          7646930|           682974|         6798372|               56383|        40416602|   40014|              55|           844049|              165|         6631|              1000|           20316|           1021795|               3644|                 5.08|              121.92|                   57.22|                1373.17|          44997|          25669|                 4173|                       328|                       12105|                  926|                   5282|                     0|\n",
      "+----------+-------+-------+-------+----------+----------+-------+--------------+----------------+--------+------------+--------------------+--------------------+--------+-----+---------+-------+-------------------+-------------------+------------------+-----------------+-----------------+----------------+--------------------+----------------+--------+----------------+-----------------+-----------------+-------------+------------------+----------------+------------------+-------------------+---------------------+--------------------+------------------------+-----------------------+---------------+---------------+---------------------+--------------------------+----------------------------+---------------------+-----------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "socialDataDf = [time: int, comments: int ... 32 more fields]\n",
       "mergedDataDf = [time: int, high: double ... 44 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 44 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Merge TradingInfo with SocialData\n",
    "\n",
    "val socialDataDf = spark.createDataFrame(socialDataInformation).sort(\"time\")\n",
    "val mergedDataDf = new_dataset.join(socialDataDf, Seq(\"time\")).cache()\n",
    "mergedDataDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finalDf = [time: int, high: double ... 45 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: int, high: double ... 45 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Saving data as Text\n",
    "\n",
    "// don't forget to delete the previous folder \n",
    "val finalDf = mergedDataDf.withColumn(\"empty\", typedLit(Seq(1)))\n",
    "finalDf.rdd.repartition(1).saveAsTextFile(data_type)\n",
    "\n",
    "println(\"All good\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
