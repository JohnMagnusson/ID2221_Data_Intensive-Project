{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "println(\"all good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is in the format:\n",
    "\n",
    "{\n",
    "\"conversionSymbol\": \"\",\n",
    "\"volumeto\": 6685400.72, \n",
    "\"high\": 9165.89, \n",
    "\"low\": 9144.6, \n",
    "\"time\": 1595030400, \n",
    "\"volumefrom\": 730.39, \n",
    "\"close\": 9151.68, \n",
    "\"open\": 9156.79, \n",
    "\"conversionType\": \"direct\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TradingInfo(\\\\,3452435.9,9181.73,9167.58,1.5951204E9,376.32,9168.16,9181.73,\\direct\\)\n",
      "TradingInfo(\\\\,3583760.67,9175.47,9162.64,1.595124E9,390.9,9165.67,9168.16,\\direct\\)\n",
      "TradingInfo(\\\\,4264728.56,9165.67,9146.8,1.5951276E9,465.73,9159.37,9165.67,\\direct\\)\n",
      "TradingInfo(\\\\,2737686.63,9163.84,9153.66,1.5951312E9,298.82,9157.3,9159.37,\\direct\\)\n",
      "TradingInfo(\\\\,3526995.72,9158.03,9149.38,1.5951348E9,385.44,9149.81,9157.3,\\direct\\)\n",
      "All good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class TradingInfo\n",
       "johnFileName = /home/john/ID2221-Labs/project/Datasets\n",
       "tradingInformation = MapPartitionsRDD[241] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createTradingInfo: (data: String)TradingInfo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[241] at map at <console>:47"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class TradingInfo (conversionSymbol: String, \n",
    "                        volumeto: Double, \n",
    "                        high: Double, \n",
    "                        low: Double, \n",
    "                        time: Double,\n",
    "                        volumefrom: Double,\n",
    "                        close: Double,\n",
    "                        open: Double,\n",
    "                        conversionType: String\n",
    "                       )\n",
    "\n",
    "def createTradingInfo(data: String): TradingInfo = {\n",
    "    val s = data.split(\", \")\n",
    "    return TradingInfo(s(0).split(\": \")(1), s(1).split(\": \")(1).toDouble, s(2).split(\": \")(1).toDouble, \n",
    "                       s(3).split(\": \")(1).toDouble, s(4).split(\": \")(1).toDouble, s(5).split(\": \")(1).toDouble, \n",
    "                       s(6).split(\": \")(1).toDouble, s(7).split(\": \")(1).toDouble, s(8).split(\": \")(1))\n",
    "}\n",
    "\n",
    "val filename = \"../Datasets/day_json.txt\"\n",
    "//val johnFileName = \"/home/john/ID2221-Labs/project/Datasets\"\n",
    "val tradingInformation = sc.textFile(filename)\n",
    "            .flatMap(l => l.split(\"},\")).\n",
    "            //map(x => x.replace(\"\\\\\", \"\")).\n",
    "            map(x => x.replace(\"[\", \"\")).\n",
    "            map(x => x.replace(\"{\", \"\")).\n",
    "            map(x => x.replace(\"\\\"\", \"\")).\n",
    "            map(createTradingInfo)\n",
    "            .cache()\n",
    "\n",
    "// THE CODE IS STUPID BUT IT WORKS \n",
    "// CONVERSION SYMBOL IS EMPTY BECAUSE I REMOVED \\ FROM IT\n",
    "\n",
    "tradingInformation.take(5).foreach(println)\n",
    "\n",
    "println(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": "Input path does not exist: file:/home/john/ID2221-Labs/Datasets/odyssey.mb.txt",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/john/ID2221-Labs/Datasets/odyssey.mb.txt",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)",
      "  at scala.Option.getOrElse(Option.scala:121)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)",
      "  at scala.Option.getOrElse(Option.scala:121)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)",
      "  at scala.Option.getOrElse(Option.scala:121)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)",
      "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)",
      "  at scala.Option.getOrElse(Option.scala:121)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)",
      "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.RDD.take(RDD.scala:1337)",
      "  ... 42 elided"
     ]
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"../Datasets/odyssey.mb.txt\")\n",
    "\n",
    "\n",
    "//rdd.take(3).foreach(println)\n",
    "\n",
    "rdd.flatMap(l => l.split(\" } \")).\n",
    "  map(x => x.replace(\"\\\\\", \"\")).\n",
    "  take(3).                \n",
    "  foreach(println)\n",
    "\n",
    "println(\"\\n . \\n \")\n",
    "\n",
    "rdd.flatMap(l => l.split(\" } \")).\n",
    "  map(x => x).//replace(\"\\\\\", \"\")).\n",
    "  take(3).                \n",
    "  foreach(println)\n",
    "\n",
    "\n",
    "println(\"\\n All good \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:17: error: not found: type SparkContext\n       val sc = new SparkContext(new SparkConf().setMaster(\"local[*]\").setAppName(\"Example\"))\n                    ^\n<console>:17: error: not found: type SparkConf\n       val sc = new SparkContext(new SparkConf().setMaster(\"local[*]\").setAppName(\"Example\"))\n                                     ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
